{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Evaluate the recognition model as whole\n",
    "## Author: Vilem Gottwald"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get features and IDs from objects predicted by the detection algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the extracted features: (48302, 7, 37)\n",
      "Shape of the object ids: (48302,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from classifier import Ensemble_ovo_ova, FeaturesExtractor\n",
    "from common import (DATA_PATH, DATASET_SPLIT_IDX, DETECTIONS_PATH, DATASET_PATH,\n",
    "                    DETECTIONS_GT_CLASSES_PATH, DETECTIONS_IOU_PATH, DETECTED_FEATURES_PATH,\n",
    "                    DETECTED_OBJECTS_IDS_PATH, COL_IDX, split_data, normalize_features)\n",
    "\n",
    "\n",
    "# Extract features and object ids form predicted detections\n",
    "dataset_extractor = FeaturesExtractor()\n",
    "try:\n",
    "    det_features, det_object_ids = dataset_extractor.load_from_saved_pred(DETECTED_FEATURES_PATH, DETECTED_OBJECTS_IDS_PATH)\n",
    "except FileNotFoundError:\n",
    "    det_features, det_object_ids = dataset_extractor.extract_from_dataset_pred(DETECTIONS_PATH)\n",
    "    dataset_extractor.save_pred(DETECTED_FEATURES_PATH, DETECTED_OBJECTS_IDS_PATH)\n",
    "\n",
    "# Normalize features\n",
    "det_features = normalize_features(det_features)\n",
    "\n",
    "print('Shape of the extracted features:', det_features.shape)\n",
    "print('Shape of the object ids:', det_object_ids.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes(probabilities):\n",
    "    \"\"\" Get classes from probabilities\n",
    "    \n",
    "    :param probabilities: array of probabilities in one-hot encoding\n",
    "    \n",
    "    :return: array of class indices\n",
    "    \"\"\"\n",
    "    classes = np.argmax(probabilities, axis=1)\n",
    "    return classes\n",
    "\n",
    "def vote_classes(probabilities, object_ids):\n",
    "    \"\"\" Vote classes for each object id\n",
    "    \n",
    "    :param probabilities: array of probabilities in one-hot encoding\n",
    "    :param object_ids: array of object ids\n",
    "    \n",
    "    :return: array of class indices\n",
    "    \"\"\"\n",
    "    objects_classes_probabilities = []\n",
    "    for object_id in np.unique(object_ids):\n",
    "        classes_probabilities = probabilities[det_object_ids == object_id]\n",
    "        classes_probabilities = np.sum(classes_probabilities, axis=0)\n",
    "        classes_probabilities = classes_probabilities/classes_probabilities.sum()\n",
    "        objects_classes_probabilities.append((object_id, classes_probabilities))\n",
    "\n",
    "    objects_classes_probabilities_dict = {object_id: classes_probabilities for object_id, classes_probabilities in objects_classes_probabilities}\n",
    "\n",
    "    grouped_probabilities = np.array([objects_classes_probabilities_dict[id] for id in det_object_ids])\n",
    "\n",
    "    voted_classes = get_classes(grouped_probabilities)\n",
    "    return voted_classes\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_confusion_matrix(gt, pred, ious, iou_threshold=0.5):\n",
    "    \"\"\" Compute confusion matrix based on iou\n",
    "    Last row  is for false positive detections under iou threshold\n",
    "    \n",
    "    :param gt: array of ground truth classes\n",
    "    :param pred: array of predicted classes\n",
    "    :param ious: array of ious\n",
    "    :param iou_threshold: iou threshold for false positive detections\n",
    "\n",
    "    :return: confusion matrix\n",
    "    \"\"\"\n",
    "    classes = np.unique(gt)\n",
    "    matrix = np.zeros((len(classes) + 1, len(classes)), dtype=np.int32)\n",
    "    \n",
    "    for i, gt_class in enumerate(classes):\n",
    "        # Compute false positive detections under iou threshold\n",
    "        mask = np.logical_and((ious < iou_threshold), (gt == gt_class))\n",
    "\n",
    "        # Add false positive detections to last row of confusion matrix\n",
    "        matrix[-1, i] = np.sum(mask)\n",
    "\n",
    "        # Remove IOU false positive detections from gt and pred\n",
    "        indexes = np.where(~mask)[0]\n",
    "        gt = gt[indexes]\n",
    "        pred = pred[indexes]\n",
    "        ious = ious[indexes]\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    for i, gt_class in enumerate(classes):\n",
    "        for j, pred_class in enumerate(classes):\n",
    "            matrix[i, j] = np.sum((gt == gt_class) & (pred == pred_class))\n",
    "\n",
    "    return matrix\n",
    "\n",
    "def get_metrics(cm, verbose=True):\n",
    "    \"\"\" Compute metrics from confusion matrix\n",
    "\n",
    "    :param cm: confusion matrix\n",
    "\n",
    "    :return: accuracy, macro_precision, macro_recall, macro_f1_score\n",
    "    \"\"\"\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    num_classes = cm.shape[1]\n",
    "    for i in range(num_classes):\n",
    "                    TP = cm[i, i]\n",
    "                    FP = np.sum(cm[:, i]) - TP\n",
    "                    FN = np.sum(cm[i, :]) - TP\n",
    "                    precision = TP / (TP + FP)\n",
    "                    recall = TP / (TP + FN)\n",
    "                    f1_score = 2 * precision * recall / (precision + recall)\n",
    "                    precisions.append(precision)\n",
    "                    recalls.append(recall)\n",
    "                    f1_scores.append(f1_score)\n",
    "\n",
    "    # Compute metrics over all classes\n",
    "    accuracy = np.sum(np.diag(cm)) / np.sum(cm)\n",
    "    macro_precision = np.mean(precisions)\n",
    "    macro_recall = np.mean(recalls)\n",
    "    macro_f1_score = np.mean(f1_scores)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Accuracy:   {accuracy*100:.2f} %\")\n",
    "        print(f\"Precision:  {macro_precision*100:.2f} %\")\n",
    "        print(f\"Recall:     {macro_recall*100:.2f} %\")\n",
    "        print(f\"F1-score:   {macro_f1_score*100:.2f} %\")\n",
    "\n",
    "    return accuracy, macro_precision, macro_recall, macro_f1_score\n",
    "\n",
    "def print_iou_metrics(gt, pred, ious):\n",
    "    \"\"\" Print metrics from confusion matrix based on iou\n",
    "    \n",
    "    :param gt: array of ground truth classes\n",
    "    :param pred: array of predicted classes\n",
    "    :param ious: array of ious\n",
    "    \"\"\"\n",
    "    cm = iou_confusion_matrix(gt, pred, ious)\n",
    "    print('Confusion_matrix:')\n",
    "    print(cm)\n",
    "    print('\\nMetrics:')\n",
    "    get_metrics(cm)\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the 5 class model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model an predict classes\n",
    "Tesnorflow warnings are caused due to the use of recurrent dropout in the LSTM. It just informs us, that slower kernel has to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Class probabilities shape: (48302, 5)\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = str(DATA_PATH / 'training' / 'models' / '5class_ensemble')\n",
    "# Load model and predict classes from features\n",
    "classif_ensemble_5 = Ensemble_ovo_ova(classes=list(range(5)))\n",
    "classif_ensemble_5.load_models(MODEL_PATH)\n",
    "\n",
    "cls_probabilities_5 = classif_ensemble_5.predict(det_features)\n",
    "print('Class probabilities shape:', cls_probabilities_5.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get predicted classes on each frame and classes voted over object frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the ground truth classes: (9660,)\n",
      "Shape of the object ious: (9660,)\n",
      "Shape of the single pred. classes: (9660,)\n",
      "Shape of the tracked pred. classes: (9660,)\n"
     ]
    }
   ],
   "source": [
    "single_clases_5 = get_classes(cls_probabilities_5)\n",
    "tracked_classes_5 = vote_classes(cls_probabilities_5, det_object_ids)\n",
    "gt_classes_5 = dataset_extractor.load_gt_classes(DETECTIONS_GT_CLASSES_PATH)\n",
    "ious = np.load(DETECTIONS_IOU_PATH)\n",
    "\n",
    "# Select test data\n",
    "_, gt_classes_5_test = split_data(gt_classes_5, DATASET_SPLIT_IDX)\n",
    "_, ious_test = split_data(ious, DATASET_SPLIT_IDX)\n",
    "_, single_clases_5_test = split_data(single_clases_5, DATASET_SPLIT_IDX)\n",
    "_, tracked_classes_5_test = split_data(tracked_classes_5, DATASET_SPLIT_IDX)\n",
    "\n",
    "print('Shape of the ground truth classes:', gt_classes_5_test.shape)\n",
    "print('Shape of the object ious:', ious_test.shape)\n",
    "print('Shape of the single pred. classes:', single_clases_5_test.shape)\n",
    "print('Shape of the tracked pred. classes:', tracked_classes_5_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single classes - Test data\n",
      "Confusion_matrix:\n",
      "[[ 317  158   12   37    5]\n",
      " [ 168 3961  473   31  196]\n",
      " [  60  354  805   88   82]\n",
      " [  23   12  101  404  166]\n",
      " [  41   29   29  219 1686]\n",
      " [  40   13    1   31  118]]\n",
      "\n",
      "Metrics:\n",
      "Accuracy:   74.25 %\n",
      "Precision:  63.54 %\n",
      "Recall:     68.25 %\n",
      "F1-score:   65.66 %\n",
      "\n",
      "Grouped classes - Test data\n",
      "Confusion_matrix:\n",
      "[[ 312  182    0   22   13]\n",
      " [  84 4530   34    0  181]\n",
      " [   0  280 1003   22   84]\n",
      " [   0    0  138  471   97]\n",
      " [   0    0    0    0 2004]\n",
      " [  40   13    1   31  118]]\n",
      "\n",
      "Metrics:\n",
      "Accuracy:   86.13 %\n",
      "Precision:  82.78 %\n",
      "Recall:     78.34 %\n",
      "F1-score:   79.86 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Single classes - Test data\")\n",
    "print_iou_metrics(gt_classes_5_test, single_clases_5_test, ious_test)\n",
    "print()\n",
    "print(\"Grouped classes - Test data\")\n",
    "print_iou_metrics(gt_classes_5_test, tracked_classes_5_test, ious_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the 3 class model\n",
    "Tesnorflow warnings are caused due to the use of recurrent dropout in the LSTM. It just informs us, that slower kernel has to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Class probabilities shape: (48302, 3)\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = str(DATA_PATH / 'training' / 'models' / '3class_ensemble')\n",
    "# Load model and predict classes from features\n",
    "classif_ensemble_3 = Ensemble_ovo_ova(classes=list(range(3)))\n",
    "classif_ensemble_3.load_models(MODEL_PATH)\n",
    "\n",
    "cls_probabilities_3 = classif_ensemble_3.predict(det_features)\n",
    "print('Class probabilities shape:', cls_probabilities_3.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get predicted classes on each frame and classes voted over object frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the ground truth classes: (9660,)\n",
      "Shape of the object ious: (9660,)\n",
      "Shape of the single pred. classes: (9660,)\n",
      "Shape of the tracked pred. classes: (9660,)\n"
     ]
    }
   ],
   "source": [
    "single_clases_3 = get_classes(cls_probabilities_3)\n",
    "tracked_classes_3 = vote_classes(cls_probabilities_3, det_object_ids)\n",
    "gt_classes_3 = dataset_extractor.load_gt_classes(DETECTIONS_GT_CLASSES_PATH, join_classes=True)\n",
    "ious = np.load(DETECTIONS_IOU_PATH)\n",
    "\n",
    "# Select test data\n",
    "_, gt_classes_3_test = split_data(gt_classes_3, DATASET_SPLIT_IDX)\n",
    "_, ious_test = split_data(ious, DATASET_SPLIT_IDX)\n",
    "_, single_clases_3_test = split_data(single_clases_3, DATASET_SPLIT_IDX)\n",
    "_, tracked_classes_3_test = split_data(tracked_classes_3, DATASET_SPLIT_IDX)\n",
    "\n",
    "print('Shape of the ground truth classes:', gt_classes_3_test.shape)\n",
    "print('Shape of the object ious:', ious_test.shape)\n",
    "print('Shape of the single pred. classes:', single_clases_3_test.shape)\n",
    "print('Shape of the tracked pred. classes:', tracked_classes_3_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single classes - Test data\n",
      "Confusion_matrix:\n",
      "[[ 344  144   41]\n",
      " [ 286 5490  442]\n",
      " [  74  121 2515]\n",
      " [  40   14  149]]\n",
      "\n",
      "Metrics:\n",
      "Accuracy:   86.43 %\n",
      "Precision:  73.77 %\n",
      "Recall:     82.04 %\n",
      "F1-score:   77.17 %\n",
      "\n",
      "Grouped classes - Test data\n",
      "Confusion_matrix:\n",
      "[[ 312  152   65]\n",
      " [  94 5839  285]\n",
      " [   0   77 2633]\n",
      " [  40   14  149]]\n",
      "\n",
      "Metrics:\n",
      "Accuracy:   90.93 %\n",
      "Precision:  83.34 %\n",
      "Recall:     83.35 %\n",
      "F1-score:   83.03 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Single classes - Test data\")\n",
    "print_iou_metrics(gt_classes_3_test, single_clases_3_test, ious_test)\n",
    "print()\n",
    "print(\"Grouped classes - Test data\")\n",
    "print_iou_metrics(gt_classes_3_test, tracked_classes_3_test, ious_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create numpy files with prediction results for visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def generate_results(dataset_path, output_path, classes, ious):\n",
    "    dataset_filepaths = sorted([os.path.join(dataset_path, file) for file in os.listdir(dataset_path)])\n",
    "\n",
    "    print(\"Generating results in directory:\", output_path)\n",
    "    curr_object_ptr = 0\n",
    "    for i, filepath in enumerate(dataset_filepaths, start=1):\n",
    "\n",
    "        frame = np.load(filepath)\n",
    "        frame_column_classes = np.full((frame.shape[0], 1),  -1)\n",
    "        frame_column_ious = np.full((frame.shape[0], 1),  -1.0)\n",
    "\n",
    "        frame_objects = np.unique(frame[:, COL_IDX[\"object_id\"]])\n",
    "\n",
    "        for object_id in frame_objects:\n",
    "            object_rows_mask = frame[:, COL_IDX[\"object_id\"]] == object_id\n",
    "\n",
    "            if frame[object_rows_mask].shape[0] < 4:\n",
    "                continue\n",
    "            \n",
    "            frame_column_classes[object_rows_mask, 0] = classes[curr_object_ptr]\n",
    "            frame_column_ious[object_rows_mask, 0] = ious[curr_object_ptr]\n",
    "            curr_object_ptr += 1\n",
    "\n",
    "        result_frame = np.hstack((frame, frame_column_classes, frame_column_ious))\n",
    "        save_filepath = os.path.join(output_path, os.path.basename(filepath))\n",
    "\n",
    "        np.save(save_filepath, result_frame)\n",
    "\n",
    "        print(f'Generating... {i} / {len(dataset_filepaths)}', end='\\r')\n",
    "    print(80*' ') # Clear line"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate results for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating results in directory: /home/xgottw07/bp/data/results/5class_ensemble_single\n",
      "                                                                                \n",
      "Generating results in directory: /home/xgottw07/bp/data/results/5class_ensemble_tracked\n",
      "                                                                                \n",
      "Generating results in directory: /home/xgottw07/bp/data/results/3class_ensemble_single\n",
      "                                                                                \n",
      "Generating results in directory: /home/xgottw07/bp/data/results/3class_ensemble_tracked\n",
      "                                                                                \n"
     ]
    }
   ],
   "source": [
    "RESULTS_DIR = str(DATA_PATH / 'results')\n",
    "\n",
    "OUT_PATH = str(DATA_PATH / 'results' / '5class_ensemble_single')\n",
    "os.mkdir(OUT_PATH)\n",
    "generate_results(DETECTIONS_PATH, OUT_PATH, single_clases_5, ious)\n",
    "\n",
    "OUT_PATH = str(DATA_PATH / 'results' / '5class_ensemble_tracked')\n",
    "os.mkdir(OUT_PATH)\n",
    "generate_results(DETECTIONS_PATH, OUT_PATH, tracked_classes_5, ious)\n",
    "\n",
    "OUT_PATH = str(DATA_PATH / 'results' / '3class_ensemble_single')\n",
    "os.mkdir(OUT_PATH)\n",
    "generate_results(DETECTIONS_PATH, OUT_PATH, single_clases_3, ious)\n",
    "\n",
    "OUT_PATH = str(DATA_PATH / 'results' / '3class_ensemble_tracked')\n",
    "os.mkdir(OUT_PATH)\n",
    "generate_results(DETECTIONS_PATH, OUT_PATH, tracked_classes_3, ious)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bp-tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
